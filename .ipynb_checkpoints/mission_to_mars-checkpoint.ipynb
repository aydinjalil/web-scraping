{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from splinter import Browser\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. NASA Mars News\n",
    "\n",
    "## Beware: Please put your own path to chromedriver.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_browser():\n",
    "    \"\"\"This function open the Google Chrome browser when called\"\"\"\n",
    "    \n",
    "    executable_path = {\"executable_path\": \"C:/Users/aydin/Downloads/chromedriver_win32/chromedriver.exe\"}\n",
    "    return Browser(\"chrome\", **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_nasa():\n",
    "    \n",
    "    \"\"\"The function returns dictionary of title and paragraph text of the latest news on NASA website\"\"\"\n",
    "    \n",
    "# Create lists for title and paragpraph.\n",
    "    _dict = {}\n",
    "    titles = []\n",
    "    para = []\n",
    "    try:\n",
    "        browser = init_browser()\n",
    "        url = \"https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest\"\n",
    "        browser.visit(url)\n",
    "\n",
    "    # Scrape page into soup\n",
    "        html = browser.html\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        time.sleep(5)\n",
    "        for title in soup.find_all('div', class_='content_title'):\n",
    "            if title.a:\n",
    "                titles.append(title.a.text)\n",
    "        for paragraph in soup.find_all('div', class_='article_teaser_body'):\n",
    "            para.append(paragraph.text)\n",
    "\n",
    "        _dict['title'] = titles[0]\n",
    "        _dict['para'] = para[0]\n",
    "        return _dict\n",
    "    except:\n",
    "        print(\" scrape_nasa() Error: Ooops!!! Something went wrong, please try again!\")\n",
    "    finally:\n",
    "        browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that titles are greater in number than the paragraph texts. Since the assignment asks to fetch only the first title and associated text we just have to visually verify whether both elements are what we are looking for.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': \"Virginia Middle School Student Earns Honor of Naming NASA's Next Mars Rover\",\n",
       " 'para': 'NASA chose a seventh-grader from Virginia as winner of the agency\\'s \"Name the Rover\" essay contest. Alexander Mather\\'s entry for \"Perseverance\" was voted tops among 28,000 entries. '}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_nasa()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. JPL Mars Space Images - Featured Image\n",
    "\n",
    "### Small function to fetch the largesize image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scrape_image():\n",
    "    \"\"\"The function returns the url of a largesize Mars picture\"\"\"\n",
    "\n",
    "#Create a list and an empty string to store image urls and the each url\n",
    "    high_res_pics = []\n",
    "    image_url = ''\n",
    "    url = \"https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\"\n",
    "    try:\n",
    "        browser = init_browser()\n",
    "        browser.visit(url)\n",
    "        html = browser.html\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        for anchors in soup.find_all('a', class_='fancybox'):\n",
    "            if 'largesize' in anchors['data-fancybox-href']:\n",
    "                image_url = 'jpl.nasa.gov' + anchors['data-fancybox-href']\n",
    "                high_res_pics.append(image_url)\n",
    "        featured_image_url = high_res_pics[0]\n",
    "        return featured_image_url\n",
    "    except: \n",
    "        print('scrape_image() Error: Ooops!!! Something went wrong, please try again!')\n",
    "    finally:\n",
    "        browser.quit()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jpl.nasa.gov/spaceimages/images/largesize/PIA23745_hires.jpg'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Mars Weather - Scraping Twitter\n",
    "\n",
    "* For some reason splinter did not work while trying to fetch the weather information from twitter. It kept delivering the Login and Sign in pages. So I needed to go back to requests the url.\n",
    "\n",
    "\n",
    "## Small function to scrape the weather data from Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_weather():\n",
    "    \"\"\"This function returns the latest Mars weather stats\"\"\"\n",
    "    \n",
    "    # list to store all available weather information. We will pick the latest one by simply saying weather_list[0]\n",
    "    weather_list = []\n",
    "    url = \"https://twitter.com/marswxreport?lang=en\" \n",
    "    try:\n",
    "        \n",
    "        html = requests.get(url).text\n",
    "        html = html.replace(\"\\n\", \", \")\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Scrape the website\n",
    "\n",
    "        for weather in soup.find_all('div', class_= \"js-tweet-text-container\"):\n",
    "            if \"InSight\" in weather.p.text:\n",
    "                weather_list.append(weather.p.getText().split('InSight')[1])\n",
    "\n",
    "        #latest Mars Weather stats\n",
    "        mars_weather = weather_list[0].split('pic.twitter.com')[0]\n",
    "        return mars_weather\n",
    "    except:\n",
    "        print(\"scrape_weather() Error: Ooops!!! Something went wrong, please try again!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' sol 452 (2020-03-05) low -94.2ºC (-137.7ºF) high -9.2ºC (15.4ºF), winds from the SSW at 6.5 m/s (14.5 mph) gusting to 19.6 m/s (43.9 mph), pressure at 6.30 hPa'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_weather()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Mars Facts - Web scraping with Pandas and BeautifulSoup\n",
    "\n",
    "## Small function to scrape information from table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mars_facts():\n",
    "    \"\"\"This function scrapes the information about Mars and returns it in html format\"\"\"\n",
    "    \n",
    "    url = \"https://space-facts.com/mars/\"\n",
    "    try: \n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content,'html.parser')\n",
    "        table = soup.find_all('table')[0] \n",
    "        df = pd.read_html(str(table))\n",
    "        mars_html_table = df[0].to_html()\n",
    "        mars_html_table\n",
    "        return mars_html_table\n",
    "    except:\n",
    "        print(\"mars_facts() Error: Ooops!!! Something went wrong, please try again!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<table border=\"1\" class=\"dataframe\">\n",
      "  <thead>\n",
      "    <tr style=\"text-align: right;\">\n",
      "      <th></th>\n",
      "      <th>0</th>\n",
      "      <th>1</th>\n",
      "    </tr>\n",
      "  </thead>\n",
      "  <tbody>\n",
      "    <tr>\n",
      "      <th>0</th>\n",
      "      <td>Equatorial Diameter:</td>\n",
      "      <td>6,792 km</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>1</th>\n",
      "      <td>Polar Diameter:</td>\n",
      "      <td>6,752 km</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>2</th>\n",
      "      <td>Mass:</td>\n",
      "      <td>6.39 × 10^23 kg (0.11 Earths)</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>3</th>\n",
      "      <td>Moons:</td>\n",
      "      <td>2 (Phobos &amp; Deimos)</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>4</th>\n",
      "      <td>Orbit Distance:</td>\n",
      "      <td>227,943,824 km (1.38 AU)</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>5</th>\n",
      "      <td>Orbit Period:</td>\n",
      "      <td>687 days (1.9 years)</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>6</th>\n",
      "      <td>Surface Temperature:</td>\n",
      "      <td>-87 to -5 °C</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>7</th>\n",
      "      <td>First Record:</td>\n",
      "      <td>2nd millennium BC</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>8</th>\n",
      "      <td>Recorded By:</td>\n",
      "      <td>Egyptian astronomers</td>\n",
      "    </tr>\n",
      "  </tbody>\n",
      "</table>\n"
     ]
    }
   ],
   "source": [
    "print(mars_facts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Mars Hemispheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_getter(titles, url_list):\n",
    "    \"\"\"This function scrapes the urls passed in url_list and fetches title and img_url, puts it into dictionary and returns list of dinctionaries\"\"\"\n",
    "    \n",
    "    hemisphere_data = []\n",
    "    hemisphere={}\n",
    "    for i in range(len(titles)):\n",
    "        response = requests.get(url_list[i])\n",
    "        hemisphere_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        for download in hemisphere_soup.find_all('div', {'class':'downloads'}):\n",
    "            hemisphere['title'] = titles[i]\n",
    "            hemisphere['img_url'] = download.ul.li.a['href']\n",
    "        \n",
    "        hemisphere_data.append(dict(hemisphere))\n",
    "        \n",
    "    return hemisphere_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small function to get images from Astropedia "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hemispheres():\n",
    "    \"\"\"This function returns the images of the Mars Hemispheres. Calls function image_getter to obtain the urls\"\"\"\n",
    "    \n",
    "    url = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "    try:\n",
    "    \n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content,'html.parser')\n",
    "\n",
    "\n",
    "        hemisphere_urls  = []\n",
    "        title_list = []\n",
    "        for items in soup.find_all('div', class_='item'):\n",
    "            title = items.div.h3.text.replace(' Enhanced', '')\n",
    "\n",
    "        # Create urls by concatenating the start and the portion of url derived from href\n",
    "\n",
    "            hemisphere_urls.append(url.split('search')[0] + items.a['href'])\n",
    "            title_list.append(title)\n",
    "\n",
    "        # Call image_getter\n",
    "\n",
    "        hemisphere_image_urls = img_getter(title_list, hemisphere_urls)\n",
    "        return hemisphere_image_urls\n",
    "    except:\n",
    "        print(\"hemispheres() Error: Ooops!!! Something went wrong, please try again!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Cerberus Hemisphere',\n",
       "  'img_url': 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/cerberus_enhanced.tif/full.jpg'},\n",
       " {'title': 'Schiaparelli Hemisphere',\n",
       "  'img_url': 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/schiaparelli_enhanced.tif/full.jpg'},\n",
       " {'title': 'Syrtis Major Hemisphere',\n",
       "  'img_url': 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/syrtis_major_enhanced.tif/full.jpg'},\n",
       " {'title': 'Valles Marineris Hemisphere',\n",
       "  'img_url': 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/valles_marineris_enhanced.tif/full.jpg'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hemispheres()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('python37': conda)",
   "language": "python",
   "name": "python37564bitpython37condae93ba7dd679f49659f306f287a61ed81"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
