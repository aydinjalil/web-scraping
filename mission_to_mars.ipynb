{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from splinter import Browser\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. NASA Mars News\n",
    "\n",
    "## Beware: Please put your own path to chromedriver.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_browser():\n",
    "    \"\"\"This function open the Google Chrome browser when called\"\"\"\n",
    "    \n",
    "    executable_path = {\"executable_path\": \"C:/Users/aydin/Downloads/chromedriver_win32/chromedriver.exe\"}\n",
    "    return Browser(\"chrome\", **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_nasa():\n",
    "    \n",
    "    \"\"\"The function returns title and paragraph text of the latest news on NASA website\"\"\"\n",
    "    \n",
    "# Create lists for title and paragpraph.\n",
    "    titles = []\n",
    "    para = []\n",
    "    \n",
    "    browser = init_browser()\n",
    "    url = \"https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest\"\n",
    "    browser.visit(url)\n",
    "    \n",
    "# Scrape page into soup\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    for title in soup.find_all('div', class_='content_title'):\n",
    "        if title.a:\n",
    "            titles.append(title.a.text)\n",
    "    for paragraph in soup.find_all('div', class_='article_teaser_body'):\n",
    "        para.append(paragraph.text)\n",
    "    print(f\"Number of titles: {len(titles)}\\nNumber of Paragraphs: {len(para)}\\n\")\n",
    "\n",
    "    print(f\"\\033[1m{titles[0]}:\\033[0m\\n{para[0]}\")\n",
    "\n",
    "    latest_title = titles[0]\n",
    "    latest_para = para[0]\n",
    "    browser.quit()\n",
    "    return latest_title, latest_para"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that titles are greater in number than the paragraph texts. Since the assignment asks to fetch only the first title and associated text we just have to visually verify whether both elements are what we are looking for.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. JPL Mars Space Images - Featured Image\n",
    "\n",
    "### Small function to fetch the largesize image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scrape_image():\n",
    "    \"\"\"The function returns the url of a largesize Mars picture\"\"\"\n",
    "\n",
    "#Create a list and an empty string to store image urls and the each url\n",
    "    high_res_pics = []\n",
    "    image_url = ''\n",
    "    \n",
    "    browser = init_browser()\n",
    "    url = \"https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\"\n",
    "    browser.visit(url)\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    for anchors in soup.find_all('a', class_='fancybox'):\n",
    "        if 'largesize' in anchors['data-fancybox-href']:\n",
    "            image_url = 'jpl.nasa.gov' + anchors['data-fancybox-href']\n",
    "            high_res_pics.append(image_url)\n",
    "    featured_image_url = high_res_pics[0]\n",
    "    browser.quit()\n",
    "    return featured_image_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Mars Weather - Scraping Twitter\n",
    "\n",
    "* For some reason splinter did not work while trying to fetch the weather information from twitter. It kept delivering the Login and Sign in pages. So I needed to go back to requests the url.\n",
    "\n",
    "\n",
    "## Small function to scrape the weather data from Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_weather():\n",
    "    \"\"\"This function returns the latest Mars weather stats\"\"\"\n",
    "    \n",
    "    # list to store all available weather information. We will pick the latest one by simply saying weather_list[0]\n",
    "    weather_list = []\n",
    "    \n",
    "    url = \"https://twitter.com/marswxreport?lang=en\" \n",
    "    html = requests.get(url).text\n",
    "    html = html.replace(\"\\n\", \", \")\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Scrape the website\n",
    "    \n",
    "    for weather in soup.find_all('div', class_= \"js-tweet-text-container\"):\n",
    "        if \"InSight\" in weather.p.text:\n",
    "            weather_list.append(weather.p.getText().split('InSight')[1])\n",
    "\n",
    "    #latest Mars Weather stats\n",
    "    mars_weather = weather_list[0].split('pic.twitter.com')[0]\n",
    "    return mars_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' sol 452 (2020-03-05) low -94.2ºC (-137.7ºF) high -9.2ºC (15.4ºF), winds from the SSW at 6.5 m/s (14.5 mph) gusting to 19.6 m/s (43.9 mph), pressure at 6.30 hPa'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_weather()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Mars Facts - Web scraping with Pandas and BeautifulSoup\n",
    "\n",
    "## Small function to scrape information from table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mars_facts():\n",
    "    \"\"\"This function scrapes the information about Mars and returns it in html format\"\"\"\n",
    "    \n",
    "    url = \"https://space-facts.com/mars/\" \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content,'html.parser')\n",
    "    table = soup.find_all('table')[0] \n",
    "    df = pd.read_html(str(table))\n",
    "    mars_html_table = df[0].to_html()\n",
    "    return mars_html_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Mars Hemispheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_getter(titles, url_list):\n",
    "    \"\"\"This function scrapes the urls passed in url_list and fetches title and img_url, puts it into dictionary and returns list of dinctionaries\"\"\"\n",
    "    \n",
    "    hemisphere_data = []\n",
    "    hemisphere={}\n",
    "    for i in range(len(titles)):\n",
    "        response = requests.get(url_list[i])\n",
    "        hemisphere_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        for download in hemisphere_soup.find_all('div', {'class':'downloads'}):\n",
    "            hemisphere['title'] = titles[i]\n",
    "            hemisphere['img_url'] = download.ul.li.a['href']\n",
    "        \n",
    "        hemisphere_data.append(dict(hemisphere))\n",
    "        \n",
    "    return hemisphere_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small function to get images from Astropedia "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hemispheres():\n",
    "    \"\"\"This function returns the images of the Mars Hemispheres. Calls function image_getter to obtain the urls\"\"\"\n",
    "    \n",
    "    url = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content,'html.parser')\n",
    "    \n",
    "    \n",
    "    hemisphere_urls  = []\n",
    "    title_list = []\n",
    "    for items in soup.find_all('div', class_='item'):\n",
    "        title = items.div.h3.text.replace(' Enhanced', '')\n",
    "    \n",
    "    # Create urls by concatenating the start and the portion of url derived from href\n",
    "    \n",
    "        hemisphere_urls.append(url.split('search')[0] + items.a['href'])\n",
    "        title_list.append(title)\n",
    "\n",
    "    # Call image_getter\n",
    "    \n",
    "    hemisphere_image_urls = img_getter(title_list, hemisphere_urls)\n",
    "    return hemisphere_image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('python37': conda)",
   "language": "python",
   "name": "python37564bitpython37condae93ba7dd679f49659f306f287a61ed81"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
